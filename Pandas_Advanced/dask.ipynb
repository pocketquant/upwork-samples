{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "# slower than the pandas \n",
    "ddf = dd.read_csv('RAW_1Week_2022_10_24to29.csv', parse_dates=['timestamp'], infer_datetime_format=True)\n",
    "# ddf = dd.from_pandas(df, npartitions=4)\n",
    "\n",
    "ddf['dollar_volume'] = ddf['price']*ddf['size']\n",
    "# ddf = ddf.dropna(subset=['u'])\n",
    "\n",
    "# ddf = ddf.drop(['conditions','exchange','tape'])\n",
    "min_val = ddf['price'].min().compute()\n",
    "max_val = ddf['price'].max().compute()\n",
    "# max_val\n",
    "\n",
    "# ddf.dtypes\n",
    "f'{min_val}..{max_val}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read in the data and create a DatetimeIndex\n",
    "# df = dd.read_csv('data.csv', parse_dates=['timestamp'], index='timestamp')\n",
    "ddf.index = ddf.index.tz_localize('US/Eastern')\n",
    "\n",
    "# Filter out all times that are after 9:30am and before 4:00pm\n",
    "ddf = ddf.loc[ddf.index.between_time('09:30', '16:00')]\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grouped =  df.groupby(pd.Grouper( key='timestamp',freq='1min'), as_index=False)\n",
    "\n",
    "grouped= grouped.apply(g,_bins, _midpoint)  \n",
    "help me parallize this code with dask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for dask how do i pick the optimal number of npartitions ? do i take into consideration my cpu model and cores? any tables for matching partition number and cpu number?\n",
    "\n",
    "The optimal number of partitions to use when working with Dask dataframes depends on a number of factors, including the size of the data, the resources available on the machine(s) where the computation will be executed, and the specific requirements of the task.\n",
    "\n",
    "In general, increasing the number of partitions can improve the parallelism of the computation, but it can also increase the overhead of distributing the data and coordinating the computation. As a result, there is often a trade-off between the level of parallelism and the overhead of the computation.\n",
    "\n",
    "To determine the optimal number of partitions, you can try experimenting with different values and measuring the performance of the computation. Some tips for choosing the number of partitions include:\n",
    "\n",
    "For small data, a smaller number of partitions (e.g. 1 or 2) may be sufficient.\n",
    "For larger data, a larger number of partitions (e.g. 8 or 16) may be needed to fully utilize the available resources.\n",
    "If the computation is I/O-bound (e.g. reading or writing data from a slow storage device), a larger number of partitions may be beneficial.\n",
    "If the computation is CPU-bound, a smaller number of larger partitions may be more efficient, as they can take advantage of the cache on the CPU.\n",
    "It is generally not necessary to match the number of partitions to the number of CPU cores on the machine. Instead, you can choose a value that is appropriate for the size and complexity of the task, and adjust it as needed based on the performance of the computation.\n",
    "\n",
    "It is also worth noting that Dask is designed to work with datasets that are too large to fit in memory, so the number of partitions should be chosen based on the size of the data, rather than the amount of available memory.\n",
    "\n",
    "For more information on choosing the number of partitions when working with Dask dataframes, you may find the following resources helpful:\n",
    "\n",
    "The Dask documentation on partitioning: https://docs.dask.org/en/latest/dataframe-best-practices.html#partitioning\n",
    "A tutorial on choosing the number of partitions in Dask: https://dask.org/docs/tutorial/partitions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read the parquet file into a Dask dataframe\n",
    "ddf = dd.read_parquet('data.parquet')\n",
    "\n",
    "# Print the Dask dataframe\n",
    "print(ddf)\n",
    "\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Read the parquet file into a Dask dataframe with 4 partitions\n",
    "ddf = dd.read_parquet('data.parquet', npartitions=4)\n",
    "\n",
    "# Print the Dask dataframe\n",
    "print(ddf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Load the data into a Pandas dataframe\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Convert the Pandas dataframe to a Dask dataframe\n",
    "ddf = dd.from_pandas(df, npartitions=4)\n",
    "\n",
    "# Print the Dask dataframe\n",
    "print(ddf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Load the data into a Dask dataframe\n",
    "df = dd.read_csv('data.csv')\n",
    "\n",
    "# Group the data by the timestamp column and apply the function to each group\n",
    "grouped = df.groupby(pd.Grouper(key='timestamp', freq='1min'), as_index=False).apply(g, _bins, _midpoint)\n",
    "\n",
    "# Print the result\n",
    "print(grouped)\n",
    "result = grouped.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will trigger the execution of the computation in parallel across the available workers in the Dask cluster. By default, Dask will use a single worker per CPU core on the local machine. You can also set up a Dask cluster using tools like dask-kubernetes, dask-yarn, or dask-cloudprovider to execute the computation on a distributed cluster of machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b54d6a16add27ca2e677aa06196fc1ec2f82f9925cd5147b5e0b38cd001f612"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
